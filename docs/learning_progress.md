# Airflow 101 Learning Path Progress

## Overview
This document tracks my progress through the **Airflow 101 (Airflow 3)** Learning Path offered by Astronomer Academy [](https://academy.astronomer.io/path/airflow-101). The learning path consists of 11 modules designed to build foundational skills in Apache Airflow 3, including core concepts, the Airflow UI, DAG creation, and scheduling. I am using the Astro CLI to run Airflow locally and explore its features.

## Current Progress
- **Modules Completed**: 3 out of 11
- **Current Focus**: Exploring the Airflow UI and Directed Acyclic Graphs (DAGs)
- **Environment**: Local development environment set up using the Astro CLI [](https://www.astronomer.io/docs/astro/cli/install-cli)
- **Date of Update**: July 14, 2025

### Completed Modules
1. **Module 1: Introduction to Apache Airflow**
   - Learned the basics of Airflow as an open-source workflow management platform.
   - Understood its role in programmatically authoring, scheduling, and monitoring workflows.
   - Key takeaway: Airflow pipelines are defined as code using Python, enabling dynamic and extensible workflows.

2. **Module 2: Setting Up a Local Development Environment**
   - Installed the Astro CLI to run Airflow locally.
   - Configured the Airflow environment and initialized the database.
   - Successfully launched the Airflow webserver and scheduler using `astro dev start`.

3. **Module 3: Navigating the Airflow UI**
   - Explored the Airflow UI to monitor DAGs and tasks.
   - Learned to interpret key components such as the DAGs view, Grid view, and Task logs.
   - Practiced triggering and monitoring a sample DAG.

### Current Activities
- **Exploring DAGs**: Understanding the structure of Directed Acyclic Graphs (DAGs), which define task dependencies and workflows in Airflow.
- **Airflow UI Interaction**: Investigating the Grid view, Graph view, and logs to monitor task execution and troubleshoot issues.
- **Next Steps**: Proceed to Module 4 to learn how to create a data pipeline in Airflow, focusing on writing and testing my first DAG.

## Tools and Setup
- **Astro CLI**: Used to set up and manage the local Airflow environment.
- **Airflow Version**: Apache Airflow 3
- **Operating System**: [Specify your OS, e.g., macOS, Windows, Linux]
- **Python Version**: [Specify your Python version, e.g., Python 3.9]
- **Repository**: [learning-airflow](https://github.com/[your-username]/learning-airflow)

## Goals
- Complete all 11 modules of the Airflow 101 Learning Path.
- Build and deploy a sample data pipeline using Airflow.
- Contribute to the Airflow community by sharing learnings or creating custom operators.

## Notes
- The Astro CLI simplifies local Airflow setup by providing a pre-configured environment with Docker.
- The Airflow UI is intuitive for monitoring workflows but requires understanding DAG structures to use effectively.
- Future updates will include code snippets of DAGs and configurations as I progress through the learning path.

## Next Update
I will update this document upon completing Module 4 or making significant progress in creating my first DAG.

---

*Last updated: July 14, 2025*
